"""
Random Forest Model for IT Anomaly Detection
============================================

Mod√®le de d√©tection d'anomalies bas√© sur Random Forest (supervis√©).
Excellent pour la classification d'anomalies lorsque des donn√©es √©tiquet√©es sont disponibles.

Avantages du Random Forest:
- Haute pr√©cision sur donn√©es √©tiquet√©es
- R√©sistant au surapprentissage
- Fournit l'importance des features
- G√®re bien les donn√©es d√©s√©quilibr√©es
- Rapide en inf√©rence

Cas d'usage typiques:
- Classification d'anomalies connues
- Validation des d√©tections d'autres mod√®les
- Analyse d'importance des features
- Production avec donn√©es historiques √©tiquet√©es

Author: MLOps Team
Date: June 2025
Version: 1.0
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.metrics import precision_recall_curve, roc_curve
import mlflow
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from base_model import BaseAnomalyModel

logger = logging.getLogger(__name__)

class RandomForestModel(BaseAnomalyModel):
    """
    Mod√®le de d√©tection d'anomalies utilisant Random Forest (supervis√©).
    
    Random Forest est optimal pour:
    - Classification d'anomalies avec donn√©es √©tiquet√©es
    - Haute pr√©cision et rappel
    - Interpr√©tabilit√© via importance des features
    - Robustesse face au bruit et aux outliers
    - Gestion des classes d√©s√©quilibr√©es
    
    Attributes:
        n_estimators (int): Nombre d'arbres dans la for√™t
        max_depth (int): Profondeur maximale des arbres
        min_samples_split (int): Minimum d'√©chantillons pour diviser un n≈ìud
        min_samples_leaf (int): Minimum d'√©chantillons dans une feuille
        max_features (str/float): Nombre de features √† consid√©rer pour chaque split
        bootstrap (bool): Utilisation du bootstrap
        class_weight (str/dict): Pond√©ration des classes
        random_state (int): Graine al√©atoire
    """
    
    def __init__(self, n_estimators: int = 100, max_depth: int = None,
                 min_samples_split: int = 2, min_samples_leaf: int = 1,
                 max_features: str = "sqrt", bootstrap: bool = True,
                 class_weight: str = "balanced", random_state: int = 42):
        """
        Initialise le mod√®le Random Forest.
        
        Args:
            n_estimators (int): Nombre d'arbres dans la for√™t
                              - 100: bon compromis (recommand√©)
                              - 200+: meilleure pr√©cision, plus lent
                              - 50: plus rapide, moins pr√©cis
            max_depth (int): Profondeur maximale des arbres
                           - None: pas de limite (peut surappendre)
                           - 10-20: bon compromis pour IT
                           - 5: mod√®le plus simple
            min_samples_split (int): √âchantillons minimum pour diviser
                                   - 2: maximum de splits (par d√©faut)
                                   - 5-10: √©vite le surapprentissage
            min_samples_leaf (int): √âchantillons minimum dans une feuille
                                  - 1: maximum de d√©tail (par d√©faut)
                                  - 2-5: √©vite le surapprentissage
            max_features (str): Features consid√©r√©es pour chaque split
                              - "sqrt": racine du nombre total (recommand√©)
                              - "log2": log2 du nombre total
                              - "auto": equivalent √† "sqrt"
                              - float: proportion des features
            bootstrap (bool): √âchantillonnage bootstrap
                            - True: avec remise (recommand√©)
                            - False: sans remise
            class_weight (str/dict): Pond√©ration des classes
                                   - "balanced": √©quilibre automatique (recommand√©)
                                   - "balanced_subsample": √©quilibre par √©chantillon
                                   - dict: poids personnalis√©s {0: 1, 1: 10}
                                   - None: pas de pond√©ration
            random_state (int): Graine al√©atoire pour reproductibilit√©
        
        Example:
            # Configuration standard pour IT
            model = RandomForestModel(n_estimators=100, max_depth=15)
            
            # Configuration haute performance
            model = RandomForestModel(n_estimators=200, max_depth=20, min_samples_split=5)
            
            # Configuration rapide
            model = RandomForestModel(n_estimators=50, max_depth=10, max_features=0.8)
        """
        super().__init__(model_name="RandomForest", model_type="supervised")
        
        # Param√®tres du mod√®le
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.bootstrap = bootstrap
        self.class_weight = class_weight
        self.random_state = random_state
        
        # M√©triques avanc√©es
        self.feature_importance_df = None
        self.cv_scores = None
        self.confusion_matrix = None
        
        # Validation des param√®tres
        self._validate_parameters()
        
        logger.info(f"RandomForest initialis√©: n_estimators={n_estimators}, "
                   f"max_depth={max_depth}, class_weight={class_weight}")
    
    def _validate_parameters(self):
        """Valide les param√®tres du mod√®le."""
        if self.n_estimators < 1:
            raise ValueError("n_estimators doit √™tre positif")
        
        if self.max_depth is not None and self.max_depth < 1:
            raise ValueError("max_depth doit √™tre positif ou None")
        
        if self.min_samples_split < 2:
            raise ValueError("min_samples_split doit √™tre >= 2")
        
        if self.min_samples_leaf < 1:
            raise ValueError("min_samples_leaf doit √™tre >= 1")
    
    def _create_model(self, **kwargs):
        """
        Cr√©e l'instance Random Forest avec les param√®tres configur√©s.
        
        Args:
            **kwargs: Param√®tres additionnels pour surcharger la configuration
        
        Returns:
            RandomForestClassifier: Instance configur√©e du mod√®le
        """
        # Fusion des param√®tres par d√©faut et additionnels
        params = {
            'n_estimators': self.n_estimators,
            'max_depth': self.max_depth,
            'min_samples_split': self.min_samples_split,
            'min_samples_leaf': self.min_samples_leaf,
            'max_features': self.max_features,
            'bootstrap': self.bootstrap,
            'class_weight': self.class_weight,
            'random_state': self.random_state,
            'n_jobs': -1,  # Utilise tous les CPUs
            'verbose': 0,  # Pas de sortie verbose
            'oob_score': True if self.bootstrap else False  # Score out-of-bag
        }
        params.update(kwargs)
        
        logger.info(f"Cr√©ation RandomForest avec param√®tres: {params}")
        
        return RandomForestClassifier(**params)
    
    def _fit_model(self, X_train, y_train):
        """
        Entra√Æne le mod√®le Random Forest.
        
        Args:
            X_train: Features d'entra√Ænement normalis√©es
            y_train: Labels d'entra√Ænement (requis pour supervis√©)
        
        Returns:
            RandomForestClassifier: Mod√®le entra√Æn√©
        
        Raises:
            ValueError: Si y_train est None (supervis√© n√©cessite des labels)
        """
        if y_train is None:
            raise ValueError("Random Forest n√©cessite des labels d'entra√Ænement (supervis√©)")
        
        logger.info(f"Entra√Ænement sur {X_train.shape[0]} √©chantillons, {X_train.shape[1]} features")
        logger.info(f"Distribution des classes: {np.bincount(y_train)}")
        
        # Entra√Ænement du mod√®le
        self.model.fit(X_train, y_train)
        
        # Calcul des m√©triques d'entra√Ænement
        if self.bootstrap and hasattr(self.model, 'oob_score_'):
            logger.info(f"Score OOB: {self.model.oob_score_:.4f}")
        
        # Sauvegarde de l'importance des features
        self._calculate_feature_importance()
        
        logger.info("Entra√Ænement Random Forest termin√©")
        
        return self.model
    
    def _predict_anomalies(self, X):
        """
        Pr√©dit les anomalies avec Random Forest.
        
        Args:
            X: Features normalis√©es pour la pr√©diction
        
        Returns:
            np.array: Pr√©dictions binaires (1=anomalie, 0=normal)
        """
        predictions = self.model.predict(X)
        return predictions
    
    def predict_proba(self, X):
        """
        Pr√©dit les probabilit√©s d'anomalies.
        
        Args:
            X: Features pour la pr√©diction
        
        Returns:
            np.array: Probabilit√©s [prob_normal, prob_anomalie]
        """
        if not self.is_fitted:
            raise ValueError("Le mod√®le doit √™tre entra√Æn√© avant les pr√©dictions")
        
        X_scaled = self.scaler.transform(X)
        probabilities = self.model.predict_proba(X_scaled)
        
        return probabilities
    
    def get_anomaly_probabilities(self, X):
        """
        Retourne uniquement les probabilit√©s d'anomalies (classe 1).
        
        Args:
            X: Features pour la pr√©diction
        
        Returns:
            np.array: Probabilit√©s d'anomalies (0.0 √† 1.0)
        """
        probas = self.predict_proba(X)
        return probas[:, 1]  # Probabilit√© de la classe anomalie
    
    def _calculate_feature_importance(self):
        """Calcule et sauvegarde l'importance des features."""
        if self.model and hasattr(self.model, 'feature_importances_'):
            feature_names = self.feature_names or [f"feature_{i}" for i in range(len(self.model.feature_importances_))]
            
            self.feature_importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': self.model.feature_importances_,
                'importance_std': np.std([tree.feature_importances_ for tree in self.model.estimators_], axis=0)
            }).sort_values('importance', ascending=False)
            
            logger.info(f"Importance des features calcul√©e pour {len(self.feature_importance_df)} features")
    
    def cross_validate(self, X, y, cv_folds=5, scoring='f1'):
        """
        Effectue une validation crois√©e approfondie.
        
        Args:
            X: Features d'entra√Ænement
            y: Labels d'entra√Ænement
            cv_folds (int): Nombre de folds pour la validation crois√©e
            scoring (str): M√©trique de scoring ('accuracy', 'f1', 'roc_auc', 'precision', 'recall')
        
        Returns:
            dict: R√©sultats de la validation crois√©e
        """
        logger.info(f"Validation crois√©e avec {cv_folds} folds, m√©trique: {scoring}")
        
        # Normalisation des donn√©es
        X_scaled = self.scaler.fit_transform(X)
        
        # Validation crois√©e stratifi√©e (pr√©serve la distribution des classes)
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        
        # Mod√®le temporaire pour la validation
        temp_model = self._create_model()
        
        # Calcul des scores
        cv_scores = cross_val_score(temp_model, X_scaled, y, cv=skf, scoring=scoring, n_jobs=-1)
        
        # M√©triques multiples
        metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']
        detailed_results = {}
        
        for metric in metrics:
            try:
                scores = cross_val_score(temp_model, X_scaled, y, cv=skf, scoring=metric, n_jobs=-1)
                detailed_results[metric] = {
                    'scores': scores,
                    'mean': scores.mean(),
                    'std': scores.std(),
                    'min': scores.min(),
                    'max': scores.max()
                }
            except Exception as e:
                logger.warning(f"Impossible de calculer {metric}: {e}")
        
        self.cv_scores = detailed_results
        
        results = {
            'primary_metric': scoring,
            'primary_scores': cv_scores,
            'primary_mean': cv_scores.mean(),
            'primary_std': cv_scores.std(),
            'detailed_metrics': detailed_results
        }
        
        logger.info(f"Validation crois√©e termin√©e - {scoring}: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
        
        return results
    
    def hyperparameter_tuning(self, X, y, param_grid=None, cv_folds=3, scoring='f1'):
        """
        Optimise les hyperparam√®tres par Grid Search.
        
        Args:
            X: Features d'entra√Ænement
            y: Labels d'entra√Ænement
            param_grid (dict): Grille des param√®tres √† tester
            cv_folds (int): Nombre de folds pour la validation
            scoring (str): M√©trique d'optimisation
        
        Returns:
            dict: R√©sultats de l'optimisation
        """
        if param_grid is None:
            # Grille par d√©faut pour optimisation
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', 0.8]
            }
        
        logger.info(f"Optimisation hyperparam√®tres avec grille: {param_grid}")
        
        # Normalisation des donn√©es
        X_scaled = self.scaler.fit_transform(X)
        
        # Configuration du Grid Search
        base_model = RandomForestClassifier(
            class_weight=self.class_weight,
            random_state=self.random_state,
            n_jobs=-1
        )
        
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=param_grid,
            cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state),
            scoring=scoring,
            n_jobs=-1,
            verbose=1,
            return_train_score=True
        )
        
        # Ex√©cution de la recherche
        grid_search.fit(X_scaled, y)
        
        # Mise √† jour des param√®tres avec les meilleurs trouv√©s
        best_params = grid_search.best_params_
        for param, value in best_params.items():
            if hasattr(self, param):
                setattr(self, param, value)
        
        results = {
            'best_params': best_params,
            'best_score': grid_search.best_score_,
            'best_estimator': grid_search.best_estimator_,
            'cv_results': grid_search.cv_results_,
            'score_improvement': grid_search.best_score_ - np.mean(grid_search.cv_results_['mean_test_score'])
        }
        
        logger.info(f"Optimisation termin√©e - Meilleur score: {grid_search.best_score_:.4f}")
        logger.info(f"Meilleurs param√®tres: {best_params}")
        
        return results
    
    def analyze_predictions(self, X_test, y_test, save_plots=False):
        """
        Analyse approfondie des pr√©dictions du mod√®le.
        
        Args:
            X_test: Features de test
            y_test: Labels de test
            save_plots (bool): Sauvegarder les graphiques
        
        Returns:
            dict: Analyse compl√®te des performances
        """
        logger.info("Analyse des pr√©dictions...")
        
        # Pr√©dictions et probabilit√©s
        predictions = self.predict(X_test)
        probabilities = self.get_anomaly_probabilities(X_test)
        
        # Matrice de confusion
        cm = confusion_matrix(y_test, predictions)
        self.confusion_matrix = cm
        
        # M√©triques d√©taill√©es
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        metrics = {
            'accuracy': (predictions == y_test).mean(),
            'precision': precision_score(y_test, predictions, zero_division=0),
            'recall': recall_score(y_test, predictions, zero_division=0),
            'f1_score': f1_score(y_test, predictions, zero_division=0),
            'auc_score': roc_auc_score(y_test, probabilities),
            'confusion_matrix': cm
        }
        
        # Analyse par seuil
        thresholds = np.arange(0.1, 1.0, 0.1)
        threshold_analysis = []
        
        for threshold in thresholds:
            thresh_predictions = (probabilities >= threshold).astype(int)
            thresh_metrics = {
                'threshold': threshold,
                'accuracy': (thresh_predictions == y_test).mean(),
                'precision': precision_score(y_test, thresh_predictions, zero_division=0),
                'recall': recall_score(y_test, thresh_predictions, zero_division=0),
                'f1_score': f1_score(y_test, thresh_predictions, zero_division=0)
            }
            threshold_analysis.append(thresh_metrics)
        
        # Identification des erreurs
        false_positives = np.where((predictions == 1) & (y_test == 0))[0]
        false_negatives = np.where((predictions == 0) & (y_test == 1))[0]
        
        analysis = {
            'basic_metrics': metrics,
            'threshold_analysis': pd.DataFrame(threshold_analysis),
            'false_positives': {
                'count': len(false_positives),
                'indices': false_positives,
                'avg_probability': probabilities[false_positives].mean() if len(false_positives) > 0 else 0
            },
            'false_negatives': {
                'count': len(false_negatives),
                'indices': false_negatives,
                'avg_probability': probabilities[false_negatives].mean() if len(false_negatives) > 0 else 0
            }
        }
        
        # Rapport de classification
        classification_rep = classification_report(y_test, predictions, output_dict=True)
        analysis['classification_report'] = classification_rep
        
        logger.info(f"Analyse termin√©e - Pr√©cision: {metrics['precision']:.3f}, "
                   f"Rappel: {metrics['recall']:.3f}, F1: {metrics['f1_score']:.3f}")
        
        return analysis
    
    def detect_anomalies_with_confidence(self, X, confidence_threshold=0.8):
        """
        D√©tecte les anomalies avec seuil de confiance.
        
        Args:
            X: Features pour la d√©tection
            confidence_threshold (float): Seuil de confiance minimum (0.5 √† 1.0)
        
        Returns:
            dict: D√©tections avec niveaux de confiance
        """
        probabilities = self.get_anomaly_probabilities(X)
        
        # Classifications avec seuils
        high_confidence_anomalies = probabilities >= confidence_threshold
        medium_confidence_anomalies = (probabilities >= 0.6) & (probabilities < confidence_threshold)
        low_confidence_anomalies = (probabilities >= 0.5) & (probabilities < 0.6)
        
        results = {
            'probabilities': probabilities,
            'high_confidence': {
                'indices': np.where(high_confidence_anomalies)[0],
                'count': np.sum(high_confidence_anomalies),
                'avg_probability': probabilities[high_confidence_anomalies].mean() if np.any(high_confidence_anomalies) else 0
            },
            'medium_confidence': {
                'indices': np.where(medium_confidence_anomalies)[0],
                'count': np.sum(medium_confidence_anomalies),
                'avg_probability': probabilities[medium_confidence_anomalies].mean() if np.any(medium_confidence_anomalies) else 0
            },
            'low_confidence': {
                'indices': np.where(low_confidence_anomalies)[0],
                'count': np.sum(low_confidence_anomalies),
                'avg_probability': probabilities[low_confidence_anomalies].mean() if np.any(low_confidence_anomalies) else 0
            }
        }
        
        return results
    
    def explain_predictions(self, X_sample, feature_names=None, top_n=5):
        """
        Explique les pr√©dictions en montrant l'influence des features.
        
        Args:
            X_sample: √âchantillon √† expliquer (single row ou multiple rows)
            feature_names: Noms des features
            top_n (int): Nombre de top features √† montrer
        
        Returns:
            dict: Explication des pr√©dictions
        """
        if feature_names is None:
            feature_names = self.feature_names or [f"feature_{i}" for i in range(X_sample.shape[1])]
        
        # Assurer que c'est un array 2D
        if X_sample.ndim == 1:
            X_sample = X_sample.reshape(1, -1)
        
        predictions = self.predict(X_sample)
        probabilities = self.get_anomaly_probabilities(X_sample)
        
        explanations = []
        
        for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
            # Importance des features pour cet √©chantillon
            sample_features = X_sample[i]
            
            # Calcul de l'influence relative (feature_value * feature_importance)
            if self.feature_importance_df is not None:
                feature_influences = sample_features * self.feature_importance_df['importance'].values
                
                # Top features influentes
                top_indices = np.argsort(np.abs(feature_influences))[-top_n:][::-1]
                
                top_features = []
                for idx in top_indices:
                    top_features.append({
                        'feature_name': feature_names[idx],
                        'feature_value': sample_features[idx],
                        'feature_importance': self.feature_importance_df.iloc[idx]['importance'],
                        'influence': feature_influences[idx]
                    })
                
                explanation = {
                    'sample_index': i,
                    'prediction': pred,
                    'probability': prob,
                    'confidence': 'High' if prob > 0.8 else 'Medium' if prob > 0.6 else 'Low',
                    'top_influencing_features': top_features
                }
                
                explanations.append(explanation)
        
        return explanations


def demonstrate_random_forest():
    """
    D√©monstration compl√®te du mod√®le Random Forest.
    """
    print("üå≥ D√âMONSTRATION RANDOM FOREST")
    print("=" * 50)
    
    # 1. Initialisation
    print("\n1Ô∏è‚É£ Initialisation du mod√®le")
    model = RandomForestModel(
        n_estimators=100,
        max_depth=15,
        class_weight='balanced',
        random_state=42
    )
    print(f"‚úÖ Mod√®le initialis√©: {model.model_name}")
    
    # 2. G√©n√©ration des donn√©es
    print("\n2Ô∏è‚É£ G√©n√©ration des donn√©es")
    data = model.generate_synthetic_data(n_samples=5000, anomaly_rate=0.05)
    X = model.prepare_features(data, include_temporal=True)
    y = data['is_anomaly']
    print(f"‚úÖ {len(data)} √©chantillons, {X.shape[1]} features")
    
    # 3. Division des donn√©es
    X_train, X_test, y_train, y_test = model.split_data(X, y, test_size=0.2)
    print(f"‚úÖ Train: {len(X_train)}, Test: {len(X_test)}")
    
    # 4. Validation crois√©e
    print("\n4Ô∏è‚É£ Validation crois√©e")
    cv_results = model.cross_validate(X_train, y_train, cv_folds=5, scoring='f1')
    print(f"‚úÖ F1-Score moyen: {cv_results['primary_mean']:.3f} ¬± {cv_results['primary_std']:.3f}")
    
    # 5. Entra√Ænement
    print("\n5Ô∏è‚É£ Entra√Ænement du mod√®le")
    training_metrics = model.fit(X_train, y_train)
    print(f"‚úÖ Entra√Ænement termin√©")
    
    # 6. √âvaluation
    print("\n6Ô∏è‚É£ √âvaluation d√©taill√©e")
    analysis = model.analyze_predictions(X_test, y_test)
    metrics = analysis['basic_metrics']
    print(f"‚úÖ Pr√©cision: {metrics['precision']:.3f}")
    print(f"‚úÖ Rappel: {metrics['recall']:.3f}")
    print(f"‚úÖ F1-Score: {metrics['f1_score']:.3f}")
    print(f"‚úÖ AUC: {metrics['auc_score']:.3f}")
    
    # 7. Importance des features
    print("\n7Ô∏è‚É£ Importance des features")
    importance = model.get_feature_importance()
    if importance is not None:
        print("Top 5 features importantes:")
        for _, row in importance.head().iterrows():
            print(f"   - {row['feature']}: {row['importance']:.4f}")
    
    # 8. D√©tection avec confiance
    print("\n8Ô∏è‚É£ D√©tection avec niveaux de confiance")
    confidence_results = model.detect_anomalies_with_confidence(X_test, confidence_threshold=0.8)
    print(f"‚úÖ Haute confiance: {confidence_results['high_confidence']['count']} anomalies")
    print(f"‚úÖ Confiance moyenne: {confidence_results['medium_confidence']['count']} anomalies")
    
    # 9. R√©sum√©
    print("\n9Ô∏è‚É£ R√©sum√© du mod√®le")
    model.summary()
    
    return model


if __name__ == "__main__":
    # D√©monstration compl√®te
    trained_model = demonstrate_random_forest()
    
    print("\nüéØ RANDOM FOREST PR√äT POUR LA PRODUCTION!")
    print("Utilisez ce mod√®le pour la classification supervis√©e d'anomalies.")
