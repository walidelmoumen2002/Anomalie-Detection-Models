"""
LSTM Model for IT Anomaly Detection
===================================

Mod√®le de d√©tection d'anomalies bas√© sur LSTM (Long Short-Term Memory).
Optimal pour l'analyse de s√©quences temporelles et la d√©tection de patterns complexes.

Avantages du LSTM:
- Capture les d√©pendances temporelles √† long terme
- D√©tecte les patterns s√©quentiels complexes
- Excellent pour les s√©ries temporelles
- Peut pr√©dire les anomalies futures
- G√®re la m√©moire s√©lective des √©v√©nements pass√©s

Cas d'usage typiques:
- Analyse de tendances temporelles
- Pr√©diction d'anomalies futures
- D√©tection de patterns s√©quentiels
- Surveillance continue en temps r√©el

Author: MLOps Team
Date: June 2025
Version: 1.0
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, TimeDistributed
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.regularizers import l1_l2
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import mlflow
import mlflow.tensorflow
import logging
import matplotlib.pyplot as plt
from base_model import BaseAnomalyModel

# Configuration TensorFlow pour √©viter les warnings
tf.get_logger().setLevel('ERROR')
logger = logging.getLogger(__name__)

class LSTMModel(BaseAnomalyModel):
    """
    Mod√®le de d√©tection d'anomalies utilisant LSTM (Deep Learning).
    
    LSTM est optimal pour:
    - Analyse de s√©quences temporelles
    - D√©tection de patterns complexes dans le temps
    - Pr√©diction d'anomalies futures
    - Capture des d√©pendances √† long terme
    - Gestion de la m√©moire des √©v√©nements pass√©s
    
    Attributes:
        sequence_length (int): Longueur des s√©quences d'entr√©e
        lstm_units (list): Nombre d'unit√©s LSTM par couche
        dropout_rate (float): Taux de dropout pour r√©gularisation
        learning_rate (float): Taux d'apprentissage
        batch_size (int): Taille des batches d'entra√Ænement
        epochs (int): Nombre d'√©poques d'entra√Ænement
        validation_split (float): Proportion pour validation
        early_stopping_patience (int): Patience pour early stopping
    """
    
    def __init__(self, sequence_length: int = 10, lstm_units: list = [50, 50],
                 dropout_rate: float = 0.2, learning_rate: float = 0.001,
                 batch_size: int = 32, epochs: int = 50, validation_split: float = 0.2,
                 early_stopping_patience: int = 10, random_state: int = 42):
        """
        Initialise le mod√®le LSTM.
        
        Args:
            sequence_length (int): Longueur des s√©quences temporelles
                                 - 10: s√©quences courtes (recommand√© pour IT)
                                 - 20-30: s√©quences moyennes
                                 - 50+: s√©quences longues (plus de contexte)
            lstm_units (list): Unit√©s LSTM par couche
                             - [50]: une couche simple
                             - [50, 50]: deux couches (recommand√©)
                             - [100, 50, 25]: architecture profonde
            dropout_rate (float): Taux de dropout (0.0 √† 0.5)
                                - 0.2: r√©gularisation mod√©r√©e (recommand√©)
                                - 0.3-0.5: r√©gularisation forte
                                - 0.0-0.1: r√©gularisation faible
            learning_rate (float): Taux d'apprentissage
                                 - 0.001: standard (recommand√©)
                                 - 0.01: apprentissage rapide
                                 - 0.0001: apprentissage lent mais stable
            batch_size (int): Taille des batches
                            - 32: compromis m√©moire/performance (recommand√©)
                            - 64-128: plus rapide si assez de m√©moire
                            - 16: pour limitations m√©moire
            epochs (int): Nombre maximum d'√©poques
                        - 50: standard (recommand√©)
                        - 100+: pour mod√®les complexes
                        - 20-30: entra√Ænement rapide
            validation_split (float): Proportion pour validation (0.0 √† 0.5)
                                     - 0.2: 20% pour validation (recommand√©)
            early_stopping_patience (int): Patience avant arr√™t pr√©matur√©
                                          - 10: patience mod√©r√©e (recommand√©)
                                          - 5: arr√™t rapide
                                          - 20: patience √©lev√©e
            random_state (int): Graine al√©atoire pour reproductibilit√©
        
        Example:
            # Configuration standard
            model = LSTMModel(sequence_length=10, lstm_units=[50, 50])
            
            # Configuration l√©g√®re
            model = LSTMModel(sequence_length=5, lstm_units=[25], dropout_rate=0.1)
            
            # Configuration avanc√©e
            model = LSTMModel(sequence_length=20, lstm_units=[100, 50, 25], 
                            dropout_rate=0.3, epochs=100)
        """
        super().__init__(model_name="LSTM", model_type="supervised")
        
        # Param√®tres du mod√®le
        self.sequence_length = sequence_length
        self.lstm_units = lstm_units if isinstance(lstm_units, list) else [lstm_units]
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.epochs = epochs
        self.validation_split = validation_split
        self.early_stopping_patience = early_stopping_patience
        self.random_state = random_state
        
        # Callbacks et historique
        self.training_history = None
        self.callbacks = None
        
        # Configuration TensorFlow
        tf.random.set_seed(random_state)
        
        # Validation des param√®tres
        self._validate_parameters()
        
        logger.info(f"LSTM initialis√©: sequence_length={sequence_length}, "
                   f"lstm_units={lstm_units}, dropout_rate={dropout_rate}")
    
    def _validate_parameters(self):
        """Valide les param√®tres du mod√®le."""
        if self.sequence_length < 2:
            raise ValueError("sequence_length doit √™tre >= 2")
        
        if not all(units > 0 for units in self.lstm_units):
            raise ValueError("Toutes les unit√©s LSTM doivent √™tre positives")
        
        if not 0.0 <= self.dropout_rate <= 0.5:
            raise ValueError("dropout_rate doit √™tre entre 0.0 et 0.5")
        
        if self.learning_rate <= 0:
            raise ValueError("learning_rate doit √™tre positif")
        
        if self.batch_size < 1:
            raise ValueError("batch_size doit √™tre positif")
    
    def _create_model(self, input_shape=None, **kwargs):
        """
        Cr√©e l'architecture LSTM.
        
        Args:
            input_shape (tuple): Forme des donn√©es d'entr√©e (sequence_length, n_features)
            **kwargs: Param√®tres additionnels
        
        Returns:
            tf.keras.Model: Mod√®le LSTM configur√©
        """
        if input_shape is None:
            # Forme par d√©faut (sera mise √† jour lors de l'entra√Ænement)
            input_shape = (self.sequence_length, 10)
        
        logger.info(f"Cr√©ation architecture LSTM: input_shape={input_shape}")
        
        # Construction du mod√®le s√©quentiel
        model = Sequential(name="LSTM_AnomalyDetector")
        
        # Couche d'entr√©e
        model.add(Input(shape=input_shape, name="input_layer"))
        
        # Couches LSTM
        for i, units in enumerate(self.lstm_units):
            return_sequences = i < len(self.lstm_units) - 1  # Retourne s√©quences sauf derni√®re couche
            
            model.add(LSTM(
                units=units,
                return_sequences=return_sequences,
                dropout=self.dropout_rate,
                recurrent_dropout=self.dropout_rate * 0.5,  # Dropout r√©current plus faible
                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),  # R√©gularisation L1+L2
                name=f"lstm_layer_{i+1}"
            ))
            
            # Dropout suppl√©mentaire entre les couches LSTM
            if i < len(self.lstm_units) - 1:
                model.add(Dropout(self.dropout_rate, name=f"dropout_{i+1}"))
        
        # Couches denses finales
        model.add(Dense(units=25, activation='relu', name="dense_1",
                       kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))
        model.add(Dropout(self.dropout_rate, name="final_dropout"))
        
        # Couche de sortie (classification binaire)
        model.add(Dense(units=1, activation='sigmoid', name="output_layer"))
        
        # Compilation du mod√®le
        optimizer = Adam(learning_rate=self.learning_rate)
        
        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        # Affichage de l'architecture
        model.summary()
        
        return model
    
    def _prepare_sequences(self, X, y=None):
        """
        Pr√©pare les donn√©es en s√©quences pour LSTM.
        
        Args:
            X: Features (n_samples, n_features)
            y: Labels optionnels (n_samples,)
        
        Returns:
            tuple: (X_sequences, y_sequences) ou X_sequences si y=None
        """
        logger.info(f"Pr√©paration des s√©quences: sequence_length={self.sequence_length}")
        
        if len(X) < self.sequence_length:
            raise ValueError(f"Donn√©es insuffisantes: {len(X)} < {self.sequence_length}")
        
        X_sequences = []
        y_sequences = [] if y is not None else None
        
        # Cr√©ation des s√©quences glissantes
        for i in range(self.sequence_length, len(X)):
            # S√©quence des features
            sequence = X[i-self.sequence_length:i]
            X_sequences.append(sequence)
            
            # Label correspondant √† la fin de la s√©quence
            if y is not None:
                y_sequences.append(y[i])
        
        X_sequences = np.array(X_sequences)
        
        if y is not None:
            y_sequences = np.array(y_sequences)
            logger.info(f"S√©quences cr√©√©es: {X_sequences.shape}, labels: {y_sequences.shape}")
            return X_sequences, y_sequences
        else:
            logger.info(f"S√©quences cr√©√©es: {X_sequences.shape}")
            return X_sequences
    
    def _setup_callbacks(self, model_save_path="best_lstm_model.h5"):
        """
        Configure les callbacks pour l'entra√Ænement.
        
        Args:
            model_save_path (str): Chemin pour sauvegarder le meilleur mod√®le
        
        Returns:
            list: Liste des callbacks configur√©s
        """
        callbacks = []
        
        # Early Stopping
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=self.early_stopping_patience,
            restore_best_weights=True,
            verbose=1,
            mode='min'
        )
        callbacks.append(early_stopping)
        
        # R√©duction du learning rate
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=self.early_stopping_patience // 2,
            min_lr=1e-6,
            verbose=1,
            mode='min'
        )
        callbacks.append(reduce_lr)
        
        # Sauvegarde du meilleur mod√®le
        checkpoint = ModelCheckpoint(
            filepath=model_save_path,
            monitor='val_loss',
            save_best_only=True,
            save_weights_only=False,
            verbose=1,
            mode='min'
        )
        callbacks.append(checkpoint)
        
        self.callbacks = callbacks
        return callbacks
    
    def _fit_model(self, X_train, y_train):
        """
        Entra√Æne le mod√®le LSTM.
        
        Args:
            X_train: Features d'entra√Ænement
            y_train: Labels d'entra√Ænement (requis pour LSTM supervis√©)
        
        Returns:
            tf.keras.Model: Mod√®le entra√Æn√©
        """
        if y_train is None:
            raise ValueError("LSTM n√©cessite des labels d'entra√Ænement (supervis√©)")
        
        logger.info(f"Pr√©paration des s√©quences pour LSTM...")
        
        # Pr√©paration des s√©quences
        X_sequences, y_sequences = self._prepare_sequences(X_train, y_train)
        
        # Cr√©ation du mod√®le avec la bonne forme d'entr√©e
        input_shape = (X_sequences.shape[1], X_sequences.shape[2])
        self.model = self._create_model(input_shape=input_shape)
        
        # Configuration des callbacks
        callbacks = self._setup_callbacks()
        
        logger.info(f"Entra√Ænement LSTM: {X_sequences.shape[0]} s√©quences, "
                   f"{X_sequences.shape[1]} pas de temps, {X_sequences.shape[2]} features")
        
        # Entra√Ænement avec validation
        history = self.model.fit(
            X_sequences, y_sequences,
            batch_size=self.batch_size,
            epochs=self.epochs,
            validation_split=self.validation_split,
            callbacks=callbacks,
            verbose=1,
            shuffle=True
        )
        
        self.training_history = history
        
        # Log des m√©triques finales
        final_metrics = {
            'final_loss': history.history['loss'][-1],
            'final_accuracy': history.history['accuracy'][-1],
            'final_val_loss': history.history['val_loss'][-1],
            'final_val_accuracy': history.history['val_accuracy'][-1],
            'epochs_trained': len(history.history['loss'])
        }
        
        logger.info(f"Entra√Ænement termin√© apr√®s {final_metrics['epochs_trained']} √©poques")
        logger.info(f"Accuracy finale: {final_metrics['final_accuracy']:.4f}")
        
        return self.model
    
    def _predict_anomalies(self, X):
        """
        Pr√©dit les anomalies avec LSTM.
        
        Args:
            X: Features normalis√©es
        
        Returns:
            np.array: Pr√©dictions binaires (1=anomalie, 0=normal)
        """
        # Pr√©paration des s√©quences
        X_sequences = self._prepare_sequences(X)
        
        # Pr√©dictions probabilistes
        probabilities = self.model.predict(X_sequences, verbose=0)
        
        # Conversion en pr√©dictions binaires
        predictions = (probabilities.flatten() > 0.5).astype(int)
        
        return predictions
    
    def predict_probabilities(self, X):
        """
        Pr√©dit les probabilit√©s d'anomalies.
        
        Args:
            X: Features pour la pr√©diction
        
        Returns:
            np.array: Probabilit√©s d'anomalies (0.0 √† 1.0)
        """
        if not self.is_fitted:
            raise ValueError("Le mod√®le doit √™tre entra√Æn√© avant les pr√©dictions")
        
        X_scaled = self.scaler.transform(X)
        X_sequences = self._prepare_sequences(X_scaled)
        
        probabilities = self.model.predict(X_sequences, verbose=0)
        
        return probabilities.flatten()
    
    def predict_sequences(self, X, return_sequences=False):
        """
        Pr√©dictions avec informations sur les s√©quences.
        
        Args:
            X: Features pour la pr√©diction
            return_sequences (bool): Retourner les s√©quences utilis√©es
        
        Returns:
            dict: Pr√©dictions avec m√©tadonn√©es
        """
        X_scaled = self.scaler.transform(X)
        X_sequences = self._prepare_sequences(X_scaled)
        
        probabilities = self.model.predict(X_sequences, verbose=0).flatten()
        predictions = (probabilities > 0.5).astype(int)
        
        # Calcul des indices correspondants dans les donn√©es originales
        original_indices = np.arange(self.sequence_length, len(X))
        
        results = {
            'predictions': predictions,
            'probabilities': probabilities,
            'original_indices': original_indices,
            'n_sequences': len(X_sequences),
            'sequence_length': self.sequence_length
        }
        
        if return_sequences:
            results['sequences'] = X_sequences
        
        return results
    
    def detect_temporal_anomalies(self, X, window_size=5, threshold=0.7):
        """
        D√©tecte les anomalies avec analyse de fen√™tres temporelles.
        
        Args:
            X: Features temporelles
            window_size (int): Taille de la fen√™tre d'analyse
            threshold (float): Seuil de probabilit√© pour anomalies
        
        Returns:
            dict: D√©tections avec analyse temporelle
        """
        prediction_results = self.predict_sequences(X, return_sequences=True)
        probabilities = prediction_results['probabilities']
        
        # Analyse par fen√™tres glissantes
        temporal_analysis = []
        
        for i in range(len(probabilities) - window_size + 1):
            window_probs = probabilities[i:i+window_size]
            
            analysis = {
                'window_start': i,
                'window_end': i + window_size - 1,
                'max_probability': np.max(window_probs),
                'mean_probability': np.mean(window_probs),
                'anomaly_count': np.sum(window_probs > threshold),
                'trend': 'increasing' if window_probs[-1] > window_probs[0] else 'decreasing',
                'volatility': np.std(window_probs)
            }
            
            temporal_analysis.append(analysis)
        
        # D√©tection d'anomalies persistantes
        persistent_anomalies = []
        consecutive_count = 0
        
        for i, prob in enumerate(probabilities):
            if prob > threshold:
                consecutive_count += 1
            else:
                if consecutive_count >= 3:  # Anomalie persistante si 3+ points cons√©cutifs
                    persistent_anomalies.append({
                        'start_index': i - consecutive_count,
                        'end_index': i - 1,
                        'duration': consecutive_count,
                        'avg_probability': np.mean(probabilities[i-consecutive_count:i])
                    })
                consecutive_count = 0
        
        results = {
            'basic_predictions': prediction_results,
            'temporal_analysis': temporal_analysis,
            'persistent_anomalies': persistent_anomalies,
            'window_size': window_size,
            'threshold': threshold
        }
        
        return results
    
    def plot_training_history(self, save_path=None):
        """
        Visualise l'historique d'entra√Ænement.
        
        Args:
            save_path (str): Chemin pour sauvegarder le graphique
        """
        if self.training_history is None:
            logger.warning("Aucun historique d'entra√Ænement disponible")
            return
        
        history = self.training_history.history
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Historique d\'entra√Ænement LSTM', fontsize=16)
        
        # Loss
        axes[0, 0].plot(history['loss'], label='Train Loss')
        axes[0, 0].plot(history['val_loss'], label='Validation Loss')
        axes[0, 0].set_title('Loss')
        axes[0, 0].set_xlabel('√âpoque')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Accuracy
        axes[0, 1].plot(history['accuracy'], label='Train Accuracy')
        axes[0, 1].plot(history['val_accuracy'], label='Validation Accuracy')
        axes[0, 1].set_title('Accuracy')
        axes[0, 1].set_xlabel('√âpoque')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # Precision
        if 'precision' in history:
            axes[1, 0].plot(history['precision'], label='Train Precision')
            axes[1, 0].plot(history['val_precision'], label='Validation Precision')
            axes[1, 0].set_title('Precision')
            axes[1, 0].set_xlabel('√âpoque')
            axes[1, 0].set_ylabel('Precision')
            axes[1, 0].legend()
            axes[1, 0].grid(True)
        
        # Recall
        if 'recall' in history:
            axes[1, 1].plot(history['recall'], label='Train Recall')
            axes[1, 1].plot(history['val_recall'], label='Validation Recall')
            axes[1, 1].set_title('Recall')
            axes[1, 1].set_xlabel('√âpoque')
            axes[1, 1].set_ylabel('Recall')
            axes[1, 1].legend()
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Graphique sauvegard√©: {save_path}")
        
        plt.show()
    
    def get_model_summary(self):
        """
        Retourne un r√©sum√© d√©taill√© du mod√®le LSTM.
        
        Returns:
            dict: Informations sur l'architecture et l'entra√Ænement
        """
        if not self.is_fitted:
            return {"error": "Mod√®le non entra√Æn√©"}
        
        # Param√®tres du mod√®le
        total_params = self.model.count_params()
        trainable_params = sum([tf.keras.backend.count_params(w) for w in self.model.trainable_weights])
        
        summary = {
            'architecture': {
                'sequence_length': self.sequence_length,
                'lstm_units': self.lstm_units,
                'dropout_rate': self.dropout_rate,
                'total_parameters': total_params,
                'trainable_parameters': trainable_params
            },
            'training': {
                'batch_size': self.batch_size,
                'learning_rate': self.learning_rate,
                'epochs_configured': self.epochs,
                'validation_split': self.validation_split
            }
        }
        
        # Historique d'entra√Ænement
        if self.training_history:
            history = self.training_history.history
            summary['training_results'] = {
                'epochs_trained': len(history['loss']),
                'final_train_loss': history['loss'][-1],
                'final_val_loss': history['val_loss'][-1],
                'final_train_accuracy': history['accuracy'][-1],
                'final_val_accuracy': history['val_accuracy'][-1],
                'best_val_loss': min(history['val_loss']),
                'best_val_accuracy': max(history['val_accuracy'])
            }
        
        return summary


def demonstrate_lstm():
    """
    D√©monstration compl√®te du mod√®le LSTM.
    """
    print("üß† D√âMONSTRATION LSTM")
    print("=" * 50)
    
    # 1. Initialisation
    print("\n1Ô∏è‚É£ Initialisation du mod√®le")
    model = LSTMModel(
        sequence_length=10,
        lstm_units=[50, 25],
        dropout_rate=0.2,
        learning_rate=0.001,
        epochs=20,  # R√©duit pour la d√©mo
        random_state=42
    )
    print(f"‚úÖ Mod√®le initialis√©: {model.model_name}")
    
    # 2. G√©n√©ration des donn√©es
    print("\n2Ô∏è‚É£ G√©n√©ration des donn√©es temporelles")
    data = model.generate_synthetic_data(n_samples=2000, anomaly_rate=0.05)
    X = model.prepare_features(data, include_temporal=True)
    y = data['is_anomaly']
    print(f"‚úÖ {len(data)} √©chantillons temporels")
    
    # 3. Division des donn√©es
    X_train, X_test, y_train, y_test = model.split_data(X, y, test_size=0.2)
    print(f"‚úÖ Train: {len(X_train)}, Test: {len(X_test)}")
    
    # 4. Entra√Ænement
    print("\n4Ô∏è‚É£ Entra√Ænement du mod√®le LSTM")
    training_metrics = model.fit(X_train, y_train)
    print(f"‚úÖ Entra√Ænement termin√©")
    
    # 5. √âvaluation
    print("\n5Ô∏è‚É£ √âvaluation sur s√©quences")
    eval_metrics = model.evaluate(X_test, y_test)
    print(f"‚úÖ Pr√©cision: {eval_metrics['accuracy']:.3f}")
    
    # 6. Pr√©dictions s√©quentielles
    print("\n6Ô∏è‚É£ Pr√©dictions sur s√©quences temporelles")
    seq_results = model.predict_sequences(X_test[:100])
    print(f"‚úÖ {seq_results['n_sequences']} s√©quences analys√©es")
    print(f"‚úÖ {np.sum(seq_results['predictions'])} anomalies d√©tect√©es")
    
    # 7. Analyse temporelle
    print("\n7Ô∏è‚É£ Analyse des patterns temporels")
    temporal_results = model.detect_temporal_anomalies(X_test[:200], window_size=5)
    print(f"‚úÖ {len(temporal_results['persistent_anomalies'])} anomalies persistantes")
    
    # 8. R√©sum√© du mod√®le
    print("\n8Ô∏è‚É£ R√©sum√© du mod√®le LSTM")
    summary = model.get_model_summary()
    print(f"‚úÖ Param√®tres totaux: {summary['architecture']['total_parameters']}")
    print(f"‚úÖ √âpoques d'entra√Ænement: {summary['training_results']['epochs_trained']}")
    
    # 9. R√©sum√© g√©n√©ral
    print("\n9Ô∏è‚É£ R√©sum√© g√©n√©ral")
    model.summary()
    
    return model


if __name__ == "__main__":
    # D√©monstration compl√®te
    trained_model = demonstrate_lstm()
    
    print("\nüéØ LSTM PR√äT POUR LA PRODUCTION!")
    print("Utilisez ce mod√®le pour l'analyse de s√©quences temporelles.")
